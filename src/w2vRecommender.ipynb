{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import word2vec\n",
    "import datetime\n",
    "import pickle\n",
    "from collections import defaultdict, OrderedDict\n",
    "from sklearn.metrics.pairwise import euclidean_distances, pairwise_distances\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import random\n",
    "from sklearn.metrics.pairwise import euclidean_distances, pairwise_distances\n",
    "from sklearn.metrics import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import normalize\n",
    "from recommender.aRecommender import ARecommender #class\n",
    "\n",
    "from recommendation.recommendation import Recommendation #class\n",
    "from recommendation.resultOfRecommendation import ResultOfRecommendation #class\n",
    "\n",
    "from datasets.ratings import Ratings #class\n",
    "\n",
    "class RecommenderW2V(ARecommender):\n",
    "\n",
    "        #def __init__(self, arguments: List[Argument]):\n",
    "        #if type(arguments) is not Arguments:\n",
    "        #    raise ValueError(\"Argument arguments is not type Arguments.\")\n",
    "\n",
    "        #self._arguments:List[Argument] = arguments\n",
    "        #Tyv*le stepane, to musi mit kazda key-value pair svoji vlastni tridu??? Kdo ma ty definice hledat? Na tohle ti staci obycejny dictionary\n",
    "        #jak rad rikam, na tohle ti sere bilej tesak - kdyztak si to prepis, ja budu pouzivat dictionary\n",
    "\n",
    "    # ratingsSum:Dataframe<(userId:int, movieId:int, ratings:int, timestamp:int)>\n",
    "    def __init__(self, arguments):\n",
    "        #arguments je dictionary, povinny parametr je cesta k souboru s CB daty\n",
    "        self._arguments = arguments\n",
    "        #\"../../../../data/cbDataOHE.txt\" nebo \"../../../../data/cbDataTFIDF.txt\"\n",
    "        #self.w2vDataPath = self._arguments[\"w2vDataPath\"]\n",
    "        self.trainVariant = self._arguments[\"trainVariant\"]\n",
    "        self.userProfiles = {}\n",
    "        \n",
    "    def getTrainVariant(self, trainDF):\n",
    "        if self.trainVariant == \"all\":\n",
    "            return trainDF\n",
    "        elif self.trainVariant == \"positive\":\n",
    "            return trainDF.loc[trainDF[Ratings.COL_RATING] >=4]\n",
    "        elif self.trainVariant == \"posneg\":\n",
    "            trainDF[Ratings.COL_MOVIEID].loc[trainDF[Ratings.COL_RATING] < 4] = -trainDF[Ratings.COL_MOVIEID]\n",
    "            return trainDF\n",
    "           \n",
    "        \n",
    "        \n",
    "    def train(self, historyDF:pd.DataFrame, ratingsDF:pd.DataFrame, usersDF:pd.DataFrame, itemsDF:pd.DataFrame):                \n",
    "        if type(ratingsDF) is not DataFrame:\n",
    "            raise ValueError(\"Argument trainRatingsDF is not type DataFrame.\")\n",
    "            \n",
    "            \n",
    "        t = self.getTrainVariant(ratingsDF)\n",
    "        t[Ratings.COL_MOVIEID] = t[Ratings.COL_MOVIEID].astype(\"str\")\n",
    "        t_sequences = t.groupby(Ratings.COL_USERID)[Ratings.COL_MOVIEID].apply(\" \".join)\n",
    "        print(t_sequences)\n",
    "        #t_sequences.set_index(Ratings.COL_USERID, inplace=True)\n",
    "        w2vTrainData = t_sequences.values.tolist()\n",
    "        \n",
    "        w = 3\n",
    "        e = 64\n",
    "        model, rev_dict, dictionary = word2vec.word2vecRun(w,e, w2vTrainData)\n",
    "        dictionary = dict([((int(i),j) if i !=\"RARE\" else (-1,j)) for i,j in dictionary.items() ])\n",
    "        rev_dict = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        self.model = model\n",
    "        self.dictionary = dictionary\n",
    "        self.rev_dict = rev_dict\n",
    "        \n",
    "        # ratingsSum:Dataframe<(userId:int, movieId:int, ratings:int, timestamp:int)>\n",
    "\n",
    "        self.ratingsGroupDF = t.groupby(Ratings.COL_USERID)[Ratings.COL_MOVIEID]\n",
    "        userProfileDF = self.ratingsGroupDF.aggregate(lambda x: list(x))\n",
    "        self.userProfiles = userProfileDF.to_dict()\n",
    "\n",
    "\n",
    "    def update(self, ratingsUpdateDF:pd.DataFrame):\n",
    "        #ratingsUpdateDF has only one row\n",
    "        ratingsUpdateDF = self.getTrainVariant(ratingsUpdateDF)\n",
    "        if ratingsUpdateDF.shape[0] > 0:\n",
    "            row = ratingsUpdateDF.iloc[0]\n",
    "            rating = row[Ratings.COL_RATING]\n",
    "            userID = row[Ratings.COL_USERID]\n",
    "            objectID = row[Ratings.COL_MOVIEID]            \n",
    "            userTrainData = self.userProfiles.get(userID,[])\n",
    "            userTrainData.append(objectID)\n",
    "            self.userProfiles[userID] = userTrainData\n",
    "        \n",
    "    \n",
    "    def resolveUserProfile(self, userProfileStrategy, userTrainData):\n",
    "        objectIDs = [int(i) for i in userTrainData]\n",
    "        w2vObjects = [self.dictionary[i] for i in objectIDs if i in self.dictionary]   \n",
    "        \n",
    "        rec = userProfileStrategy\n",
    "        print(rec)\n",
    "        if (len(w2vObjects) > 0):\n",
    "            if (rec == \"mean\") | (rec == \"max\"):\n",
    "                weights = [1.0] * len(w2vObjects)\n",
    "            elif rec == \"last\":\n",
    "                w2vObjects = w2vObjects[-1:]\n",
    "                weights = [1.0]\n",
    "            elif rec == \"window3\":\n",
    "                w2vObjects = w2vObjects[-3:]\n",
    "                weights = [1 / len(w2vObjects) * i for i in range(1, (len(w2vObjects) + 1))]\n",
    "            elif rec == \"window5\":\n",
    "                w2vObjects = w2vObjects[-5:]\n",
    "                weights = [1 / len(w2vObjects) * i for i in range(1, (len(w2vObjects) + 1))]\n",
    "            elif rec == \"window10\":\n",
    "                w2vObjects = w2vObjects[-10:]\n",
    "                weights = [1 / len(w2vObjects) * i for i in range(1, (len(w2vObjects) + 1))]\n",
    "            \n",
    "            if rec == \"max\":\n",
    "                agg = np.max\n",
    "            else:\n",
    "                agg = np.mean\n",
    "                \n",
    "            print((w2vObjects, weights, agg))\n",
    "            return (w2vObjects, weights, agg)\n",
    "        \n",
    "\n",
    "        return ([],[],\"\")\n",
    "\n",
    "        \n",
    "    def recommend(self, userID:int, numberOfItems:int, userProfileStrategy):\n",
    "        userTrainData = self.userProfiles.get(userID,[])\n",
    "        w2vObjects, weights, aggregation = self.resolveUserProfile(userProfileStrategy,userTrainData)\n",
    "        simList = []\n",
    "        \n",
    "        #provedu agregaci dle zvolenÃ© metody\n",
    "        if len(w2vObjects)>0:        \n",
    "            embeds = self.model[w2vObjects]\n",
    "            results = 1 - pairwise_distances(embeds, self.model, metric=\"cosine\")\n",
    "            \n",
    "            weights = np.asarray(weights).reshape((-1, 1))\n",
    "            results = results * weights\n",
    "            results = aggregation(results, axis=0)\n",
    "            \n",
    "            print(type(results))\n",
    "            #check for a variant with negative preference (only positive objects recommended)\n",
    "            #approximative solution - might result in less objects\n",
    "            resultList = (-results).argsort()[0:(numberOfItems*3)] \n",
    "            resultingOIDs = [self.rev_dict[i] for i in resultList if self.rev_dict[i] > 0] \n",
    "            resultingOIDs = resultingOIDs[0:numberOfItems]\n",
    "            resultList = [self.dictionary[i] for i in resultingOIDs]\n",
    "\n",
    "            #print(results[resultList])\n",
    "            \n",
    "            #normalize scores into the unit vector (for aggregation purposes)\n",
    "            #!!! tohle je zasadni a je potreba provest normalizaci u vsech recommenderu - teda i pro most popular!\n",
    "            finalScores = results[resultList]\n",
    "            finalScores = normalize(np.expand_dims(finalScores, axis=0))[0,:]\n",
    "            \n",
    "            return ResultOfRecommendation(resultingOIDs, finalScores.tolist())\n",
    "\n",
    "        return ResultOfRecommendation([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = RecommenderW2V({\"trainVariant\":\"posneg\"})#zahrnuje negativni rating\n",
    "#rec = RecommenderW2V({\"trainVariant\":\"positive\"})#pouze pozitivni rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from pandas.core.series import Series #class\n",
    "\n",
    "from recommender.description.recommenderDescription import RecommenderDescription #class\n",
    "\n",
    "from recommender.aRecommender import ARecommender #class\n",
    "\n",
    "from recommender.recommenderTheMostPopular import RecommenderTheMostPopular #class\n",
    "from recommender.dummy.recommenderDummyRedirector import RecommenderDummyRedirector #class\n",
    "\n",
    "from datasets.ratings import Ratings #class\n",
    "from datasets.rating import Rating #class\n",
    "\n",
    "from datasets.users import Users #class\n",
    "\n",
    "from portfolio.portfolioDescription import PortfolioDescription #class\n",
    "from portfolio.portfolio import Portfolio #class\n",
    "\n",
    "from aggregation.aggregationDescription import AggregationDescription #class\n",
    "from aggregation.aggrDHont import AggrDHont #class\n",
    "\n",
    "from recommendation.resultOfRecommendation import ResultOfRecommendation #class\n",
    "\n",
    "from simulation.evaluationTool.simplePositiveFeedback import SimplePositiveFeedback #class\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.core.frame import DataFrame #class\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsFile: str = \"../../..\" + os.sep + \"data\" + os.sep +  \"ratings.dat\"\n",
    "\n",
    "ratingsDF: DataFrame = pd.read_csv(ratingsFile, sep=';', header=None)\n",
    "ratingsDF.columns = [Ratings.COL_USERID, Ratings.COL_MOVIEID, Ratings.COL_RATING, Ratings.COL_TIMESTAMP]\n",
    "ratingsDFTrain = ratingsDF.iloc[0:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpesk\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\lpesk\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:63: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId\n",
      "1      1193 -661 -914 3408 2355 -1197 1287 2804 594 9...\n",
      "2      1357 3068 1537 -647 2194 648 2268 -2628 -1103 ...\n",
      "3      3421 -1641 -648 1394 -3534 104 2735 1210 -1431...\n",
      "4      3468 -1210 2951 1214 1036 260 2028 480 -1196 1...\n",
      "5      2987 2333 1175 -39 -288 2337 1535 1392 -2268 -...\n",
      "                             ...                        \n",
      "327    3789 1248 -1179 1250 -3865 1251 1252 1254 722 ...\n",
      "328    -648 589 -736 1198 -667 -3020 -1356 -3107 -137...\n",
      "329    2987 -1175 1179 -719 -648 -3861 1320 1321 -386...\n",
      "330    -3863 1250 -3793 1256 3798 -3948 1262 -2138 11...\n",
      "331    1179 647 648 3863 -2125 -2126 1180 3868 3869 -...\n",
      "Name: movieId, Length: 331, dtype: object\n",
      "Creating Dictionary\n",
      "Actual vocabulary size:5351\n",
      "Loss at step 10000 : 0.732938826084137\n",
      "Loss at step 20000 : 0.5157715082168579\n",
      "Loss at step 30000 : 0.6451883912086487\n",
      "Loss at step 40000 : 1.3929405212402344\n",
      "Loss at step 50000 : 1.725585699081421\n",
      "Nearest to 2858: -2008, 3521, -1531, 1686, -1194,\n",
      "Nearest to 480: -3623, -2852, 1605, -1502, 3659,\n",
      "Nearest to 1242: -3011, -217, 957, 2331, 606,\n",
      "Nearest to -1976: 3585, 3340, -2414, 1063, -2359,\n",
      "Loss at step 60000 : 0.16830205917358398\n",
      "Loss at step 70000 : 1.0936745405197144\n",
      "Loss at step 80000 : 0.9712623953819275\n",
      "Loss at step 90000 : 0.8601449728012085\n",
      "Loss at step 100000 : 0.3847847878932953\n",
      "Nearest to 2858: -2008, 3521, -1531, 1686, -1194,\n",
      "Nearest to 480: -3623, -2852, 1605, -1502, 3659,\n",
      "Nearest to 1242: -217, 957, 2331, -2568, 606,\n",
      "Nearest to -1976: 3585, 3340, -2414, 1063, -2359,\n",
      "Loss at step 110000 : 0.3259020149707794\n",
      "Loss at step 120000 : 0.3425358533859253\n",
      "Loss at step 130000 : 3.609292507171631\n",
      "Loss at step 140000 : 2.85530948638916\n",
      "Loss at step 150000 : 0.2939438819885254\n",
      "Nearest to 2858: -2008, 3521, -1531, 1686, -1194,\n",
      "Nearest to 480: -3623, -2852, 1605, -1502, 3659,\n",
      "Nearest to 1242: -217, 957, 2331, 606, 3399,\n",
      "Nearest to -1976: 3585, 3340, -2414, 1063, -2359,\n",
      "Loss at step 160000 : 0.27572426199913025\n",
      "Loss at step 170000 : 0.08200286328792572\n",
      "Loss at step 180000 : 0.5975481271743774\n",
      "Loss at step 190000 : 1.1225041151046753\n",
      "Loss at step 200000 : 1.871127963066101\n",
      "Nearest to 2858: -2008, 3521, -1531, 1686, -1194,\n",
      "Nearest to 480: -3623, -2852, 1605, -1502, 3659,\n",
      "Nearest to 1242: -217, 957, 2331, 606, 3219,\n",
      "Nearest to -1976: 3585, 3340, -2414, 1063, -2359,\n"
     ]
    }
   ],
   "source": [
    "rec.train(pd.DataFrame(),ratingsDFTrain,pd.DataFrame(),pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rec.userProfiles[331])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpesk\\.conda\\envs\\tf\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\lpesk\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50001</th>\n",
       "      <td>331</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>984152511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating  timestamp\n",
       "50001     331     5000       2  984152511"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsDFUpdate = ratingsDF.iloc[50001:50002]\n",
    "ratingsDFUpdate.iloc[0,1]=5000 #test jak si poradi s neexistujicimi vstupy\n",
    "ratingsDFUpdate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lpesk\\.conda\\envs\\tf\\lib\\site-packages\\ipykernel_launcher.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "rec.update(ratingsDFUpdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rec.userProfiles[331])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5351, 64)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.model.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max\n",
      "([238, 1215, 54, 1084, 1628, 539, 2855, 341, 1185, 504, 1546, 34, 242, 43, 521, 1464, 372, 474, 312, 794, 252, 413, 1109, 4152, 3983, 1623, 623, 137, 1094, 819, 28, 1650, 204, 26, 109, 2, 84, 5, 360, 11, 1393, 920, 703, 605, 22, 2539, 216, 3012, 1532, 2805, 5344, 2096, 2152, 2712, 110, 3878, 538, 72, 187, 674, 826, 2275, 388, 194, 812, 231, 1776, 1346, 2117, 3322, 2351, 36, 394, 715, 3368, 295, 714, 89, 2797, 345, 3828, 3379, 5345, 860, 531, 1940, 232, 145, 1598, 50, 164, 5346, 547, 1519, 41, 3135, 2688, 254, 155, 563, 4154, 3516, 325, 142, 1854, 1909, 456, 1748, 151, 844, 1612, 5347, 5348, 1007, 2636, 485, 797, 264, 2776, 1237, 934, 5349, 1116, 1238, 224, 117, 3274, 593, 764, 503, 780, 1298, 42, 2033, 1131, 179, 1276, 5350, 1369, 2415, 95, 2961, 1112, 495], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], <function amax at 0x000002C6717CC1F8>)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recommendation.resultOfRecommendation.ResultOfRecommendation at 0x2c6a5c4a948>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec.recommend(331, 50, \"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
